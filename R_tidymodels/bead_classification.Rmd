---
title: "Bead Classification"
output:
  html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This dataset includes 80x80pixel images of polystyrene microspheres used to calibrate instruments like flow cytometers or microscopes. The dataset is comprised of 4 microsphere sizes: 4.5um, 6um, 10um, and 15um microspheres.For Classification, class labels include both in-focus (if) and out-of-focus (oof) images of each size, as well as debris and background noise picked up on the digital microscope.

This analysis will be completed using the tidymodels package - see https://www.tmwr.org for an introduction to the package. Tidymodels uses tidyverse concepts, which are designed to be a better interface for common tasks using R.


```{r}
library(tidyverse)
library(tidymodels)
library(skimr)
library(tidytext)
library(themis)
library(vip)
library(lemon)

theme_minimal_new <- function(..., base_size = 11) {
  theme_minimal(base_size = base_size) %+replace%
    theme(
      panel.grid.major = element_blank(), 
      panel.grid.minor = element_blank(),
      strip.background = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_line(), 
      axis.text = element_text(colour = "black", size = base_size),
      axis.title = element_text(colour = "black", size = base_size + 1),
      axis.line = element_line()
    )
}

```

<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

First, lets read the cleaned-up dataframe provided by Ben and investigate the characteristics and distribution of the variables:
```{r}
df <- read_tsv("bead_df.tsv") %>% 
  select(-patchID, -patchsize, -object_size) %>% 
  mutate(celltype = as_factor(celltype))

skim(df)

```


<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

Our aim is to use Random Forest classification to predic the 'celltype', i.e. bead size, background, and debris. We see that the outcome classes are unbalanced.
```{r}
df %>% 
  count(celltype)

```




Below is a workflow for PCA - this can provide insight into important varaibles and how the variables seperate the outcome classes of interest. Recall that PC1 will explain the most variation in the dataset.
```{r}
pca_rec <- recipe(~., data = df) %>%
  update_role(celltype, new_role = "id") %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)

tidied_pca <- pca_prep %>% 
  tidy(2)

tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  group_by(component) %>%
  top_n(8, abs(value)) %>%
  ungroup() %>%
  mutate(terms = reorder_within(terms, abs(value), component)) %>%
  ggplot(aes(abs(value), terms, fill = value > 0)) +
  geom_col() +
  scale_fill_brewer(palette = "Set1") +
  facet_rep_wrap(~component, scales = "free_y", repeat.tick.labels = TRUE)+
  scale_y_reordered() +
  labs(
    x = "\nAbsolute value of contribution",
    y = NULL, fill = "Positive?"
  ) +
  theme_minimal_new(base_size = 10) +
  theme(
    strip.text = element_text(size = 10)
  )


```



Below, we observe that the 4.5 uM and 6 uM sizes might be the most difficult to distinguish with the predictor variables. We also see overlap between the background and debris classes. 
```{r}
juice(pca_prep) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = celltype), size = 2, shape = 16) +
  scale_color_brewer(palette = "Set1") +
  labs(color = NULL) +
  theme_minimal_new() +
  xlab("\nPC1") +
  ylab("PC2\n") +
  scale_x_continuous(expand = c(0,0), limits = c(-10, 20), breaks = c(-10, -5, 0, 5, 10, 15, 20)) +
  scale_y_continuous(expand = c(0,0), limits = c(-5, 15), breaks = c(-5, 0, 5, 10, 15))

```

```{r}
sdev <- pca_prep$steps[[2]]$res$sdev
percent_variation <- sdev^2 / sum(sdev^2)

tibble(
  component = unique(tidied_pca$component),
  percent_var = percent_variation
) %>%
  mutate(component = fct_inorder(component)) %>%
  filter(component %in% c("PC1", "PC2", "PC3", "PC4")) %>% 
  ggplot(aes(component, percent_var)) +
  geom_col() +
  scale_y_continuous(
    #labels = scales::percent_format(), 
    expand = c(0,0), limits = c(0, 0.6), 
    breaks = c(0, 0.2, 0.4, 0.6)
  ) +
  coord_flip() +
  labs(x = NULL, y = "\nVariance explained") +
  theme_minimal_new()

```




Lets now work on building our Random Forest classifier. First, we split the data into training and testing subsets - 80% of the data into the training dataset and 20% into the testing dataset.
```{r}
set.seed(234)
beads_split <- initial_split(df, strata = celltype, prob = 0.80)

beads_train <- training(beads_split)
beads_test <- testing(beads_split)

print(dim(beads_train))
print(dim(beads_test))

```




With tidymodels we use recipes - A recipe is an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done.  For example, we can use step_normalize() to center and scale any predictors selected in the step. When we call prep(recipe, training), this function estimates the required means and standard deviations from the data in the training argument. The transformations specified by each step are also sequentially executed on the data set. Again using normalization as the example, the means and variances are estimated and then used to standardize the columns. 
As steps are estimated by prep, these operations are applied to the training set. The bake function applies the preprocessing steps to new datasets, such as the test sets. The juice function will return variables from the processed training set. There are also high level functions (like workflows) that handle these phases automatically - we will use them below. In these cases, we do not have to manually use prep() and bake() to include a recipe in the modeling process. Here we "prep", and "juice", to demonstrate that the SMOTE oversampling is balancing the outcome classes.
```{r}
beads_recipe <- recipe(celltype ~ ., data = beads_train) %>%
  step_normalize(all_predictors()) %>% # Center and scale (i.e. normalize) all the predictors.
  step_smote(celltype) # Implement SMOTE oversampling so that the cell types are balanced

beads_prep <- prep(beads_recipe)

juiced <- juice(beads_prep)

juiced %>% 
  count(celltype)

```



Lets tune a random forest classifier and buiild a tidymodels workflow for the recipe. A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests. For example, if you have a recipe and parsnip model, these can be combined into a workflow. The advantages are:

* You don’t have to keep track of separate objects in your workspace.
* The recipe prepping and model fitting can be executed using a single call to fit().
* If you have custom tuning parameter settings, these can be defined using a simpler interface when combined with tune.
* Workflows can add post-processing operations, such as modifying the probability cutoff for two-class models.




We create a set of cross-validation resamples to use for tuning the mtry and min_n parameters of the random forest model.
```{r}
tune_spec <- rand_forest(
  mtry = tune(),
  trees = 100,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")

tune_wf <- workflow() %>%
  add_recipe(beads_recipe) %>%
  add_model(tune_spec)

# Now it’s time to tune the hyperparameters for a random forest model. 
set.seed(234)
beads_folds <- vfold_cv(beads_train)

#We can’t learn the right values when training a single model, but we can train a whole bunch of models and see which ones turn out best. We can use parallel processing to make this go faster, since the different parts of the grid are independent.
doParallel::registerDoParallel()

set.seed(345)
tune_results <- tune_grid(
  tune_wf,
  resamples = beads_folds
)

```



How did this turn out? Let’s look at the results using the accuracy metric.
```{r}
tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  scale_color_brewer(palette = "Set1") +
  facet_rep_wrap(~parameter, repeat.tick.labels = TRUE) +
  xlab("") +
  ylab("Accuracy\n") +
  theme_minimal_new() +
  scale_x_continuous(expand = c(0,0), limits = c(0,40), breaks = c(0, 10, 20, 30, 40)) +
  scale_y_continuous(expand = c(0,0), limits = c(0.950,0.975), breaks = c(0.950, 0.955, 0.960, 0.965, 0.970, 0.975)) +
  theme(
    strip.text = element_text(size = 12),
    panel.spacing = unit(30, "points")
  )

```




The grid approach above did not involve every combination of min_n and mtry but it provides an idea of what parameter ranges are best. We can get a better handle on the hyperparameters by tuning one more time, this time using regular_grid(). In this approach we set ranges of hyperparameters we want to try, based on the results from our initial tune.
```{r}
rf_grid <- grid_regular(
  mtry(range = c(6, 12)),
  min_n(range = c(2, 10)),
  levels = 6
)

rf_grid

```

```{r}
set.seed(456)
tune_results <- tune_grid(
  tune_wf,
  resamples = beads_folds,
  grid = rf_grid
)

```




Lets visualzie the results of the tuning for each of the two paramters, using accuracy as our metric of interest:
```{r}
tune_results %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = paste("min_n:", min_n)) %>%
  ggplot(aes(mtry, mean)) +
  geom_line() +
  facet_rep_wrap(~min_n, repeat.tick.labels = TRUE) +
  scale_color_brewer(palette = "Set1") +
  ylab("Accuracy\n") +
  xlab("\nmtry") +
  scale_y_continuous(expand = c(0,0), breaks = c(0.966, 0.971, 0.976), limits = c(0.966, 0.976)) + 
  scale_x_continuous(expand = c(0,0), breaks = c(6, 8, 10, 12)) + 
  theme_minimal_new() +
  theme(
    strip.text = element_text(size = 11),
    panel.spacing = unit(30, "points")
  )

```




Now select the best model after tuning based on the accuracy metric and update the wokflow with the model. Recall, the workflow consists of a recipe and a model. When we call fit() on the workflow, the steps of the workflow are executed. Printing the model shows us what those processing steps are and what model we are using.
```{r}
best_rf <- select_best(tune_results, metric = "accuracy")

rf_wf <- finalize_workflow(
  tune_wf,
  best_rf
)

rf_wf

```




Fit the final best model to the training set and evaluate the test set:
```{r}
rf_wf_fit <- fit(rf_wf, beads_train)

results <- predict(rf_wf_fit, beads_test) %>% 
  bind_cols(beads_test %>% select(celltype))

results

```


```{r}
metrics(results, truth = celltype, estimate = .pred_class)

```




Lets investigate the confusion matrix. We see that the model misclassified several 6 uM images as 4.5 uM and misclassified a few debris images as background. This appears to agree with what we observed in our PCA analysis.
```{r}
results %>%
  conf_mat(celltype, .pred_class) %>% 
  autoplot(type = "heatmap") +
  xlab("\nTruth") +
  ylab("Prediction\n") +
  theme_minimal_new() +
  theme(
      axis.line = element_blank()
  )

```




Now lets look at the predictors that the random forest identitied as most important for classification. How does this compare to the PCA anlaysis?
```{r}
finalize_model(
  tune_spec,
  best_rf
) %>% 
  set_engine("ranger", importance = "permutation") %>%
  fit(celltype ~ ., data = juice(beads_prep)) %>%
  vip(geom = "col") +
  ylab("\nImportance") +
  scale_y_continuous(expand = c(0.0, 0.0), limits = c(0, 0.25), breaks = c(0.0, 0.125, 0.25)) +
  theme_minimal_new()

```

